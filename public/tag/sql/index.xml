<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SQL | </title>
    <link>/tag/sql/</link>
      <atom:link href="/tag/sql/index.xml" rel="self" type="application/rss+xml" />
    <description>SQL</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 20 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu8001a07fc39adc97ccd024efda7dbc16_12171_512x512_fill_lanczos_center_2.png</url>
      <title>SQL</title>
      <link>/tag/sql/</link>
    </image>
    
    <item>
      <title>Scraping Durham Property Records: Part I</title>
      <link>/post/blog2/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/blog2/</guid>
      <description>&lt;p&gt;This week I decided to write a pretty simple webscraper to collect some property records from the 
&lt;a href=&#34;https://www.dconc.gov/government/departments-f-z/tax-administration/real-estate-appraisal/real-property-record-search&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Durham County website&lt;/a&gt;. I looked to see if they had an API, but I couldn&amp;rsquo;t find one, so this seemed like an opportunity to write a short blog about building a quick webscraper (reading for 
&lt;a href=&#34;#reading-list&#34;&gt;technical resources&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This script will build a database of property values and information that I can use for future analysis and projects (thoughts on 
&lt;a href=&#34;#future-uses&#34;&gt;future uses&lt;/a&gt;). Since I view this as &amp;lsquo;human data&amp;rsquo;, at least in some part, I&amp;rsquo;m only going to scrape non-identifiable data (articles on 
&lt;a href=&#34;#reading-list&#34;&gt;ethics in data mining&lt;/a&gt;); I won&amp;rsquo;t be collecting the physical address or owner&amp;rsquo;s name for each property. While I will go over the code here, you can download the python code from 
&lt;a href=&#34;https://github.com/pswhitehead/Durham-Property-Webscraper&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my repository here&lt;/a&gt;.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id=&#34;main-goal&#34;&gt;Main Goal&lt;/h3&gt;
&lt;p&gt;The main goal today when building this webscraper is to load and collect relevant property record data from each parcel in Durham county, from the county website, and then input that data in to a &lt;code&gt;PostgreSQL&lt;/code&gt; database. Below I will walk through each section of the code to do this.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4 id=&#34;step-1&#34;&gt;Step 1&lt;/h4&gt;
&lt;p&gt;We need to import all the libraries we need for this project. I am using &lt;code&gt;Selenium&lt;/code&gt; (
&lt;a href=&#34;https://selenium-python.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;), as the website I&amp;rsquo;m scraping from needs to load up the webpage for us to eb able to scrape the information (link about that here). I also chose to use &lt;code&gt;psycopg2&lt;/code&gt; (
&lt;a href=&#34;https://www.psycopg.org/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;) to interact with the &lt;code&gt;PostgreSQL&lt;/code&gt; database I created for this project. If you need to install &lt;code&gt;Selenium&lt;/code&gt; ir &lt;code&gt;psycopg2&lt;/code&gt;, it&amp;rsquo;s pretty easy to do via &lt;code&gt;Anaconda&lt;/code&gt; (
&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;####Import the libraries we need####
import numpy as np
from selenium import webdriver
from bs4 import BeautifulSoup
import time
import re
from selenium.webdriver.firefox.options import Options
import psycopg2
from psycopg2 import sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4 id=&#34;step-2&#34;&gt;Step 2&lt;/h4&gt;
&lt;p&gt;Once we have the libraries loaded up we are going to write the functions we need both to interact with the SQL database as well as scrape the data from the county website.&lt;/p&gt;
&lt;p&gt;In the SQL database I created, I have a table &amp;ndash; &lt;code&gt;scrapeprog&lt;/code&gt; &amp;ndash; that contains all the url IDs and a status variable for each, currently set to 2 which will represent a &amp;lsquo;pending&amp;rsquo; status here. This first SQL call function is going to allow us to run the .py script without needing to give an input by querying the &lt;code&gt;scrapeprog&lt;/code&gt; table in our SQL database to see which url ID we haven&amp;rsquo;t tried to scrape yet.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##To query what we need to scrape yet##
def findsql():    
    #Set up a connection to the PostgresSQL server we&#39;ll write data to
    conn = psycopg2.connect(host=&amp;quot;localhost&amp;quot;,database=&amp;quot;durhamprop&amp;quot;, user=&amp;quot;postgres&amp;quot;, password=&amp;quot;postgres&amp;quot;)
    cur = conn.cursor()
    
    #Write an sql query
    query = sql.SQL(&amp;quot;SELECT * FROM {table} WHERE {key} = 2 LIMIT 1&amp;quot;).format(
        table=sql.Identifier(&#39;scrapeprog&#39;),
        key=sql.Identifier(&#39;status&#39;))
    
    #execute query
    cur.execute(query)
    return cur.fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second SQL call function will allow us to insert and edit data in the SQL database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##To make sql calls to insert and change data##
def callsql(command, data = None):
    #Set up a connection to the PostgresSQL server we&#39;ll write data to
    conn = psycopg2.connect(host=&amp;quot;localhost&amp;quot;,database=&amp;quot;durhamprop&amp;quot;, user=&amp;quot;postgres&amp;quot;, password=&amp;quot;postgres&amp;quot;)
    cur = conn.cursor()
    
    #execute the query
    if data == None:
        cur.execute(command)
    else:
        cur.execute(command, data)
    conn.commit()  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The third function I wrote allows us to load a webpage from the county&amp;rsquo;s property records site and scrape the data we want to put in the SQL database&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##To scrape the data from each property##
def getpropertydata(i):  
    #Setp up the website we&#39;ll scrape data from
    url = [&amp;quot;https://property.spatialest.com/nc/durham/#/property/&amp;quot;+str(i)]
    
    #Set up the Selenium web driver and then call the URL
    options = Options()
    options.headless = True #I don&#39;t want to open a physical window each tme
    driver = webdriver.Firefox(options=options)
    driver.get(url[0])
    time.sleep(2) #Because we are loading the page, I wait a few seconds before the next command
    
    if driver.current_url == url[0]: #check the status isn&#39;t 404  or redirected before we scrape the info
        #Parse the html
        soup_ID = BeautifulSoup(driver.page_source, &#39;html.parser&#39;)
        
        #Find the key info (ki)
        ki0 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[9]))[0] #landuse
        ki1 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[13]))[0] #subdiv
        ki2 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[21]))[0] #saledate
        ki3 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[23]))[0] #saleprice
        
        #Find the buildinfo (bi)
        bi0 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[-1]))[0] #value
        bi1 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[4]))[0] #year built
        bi2 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[13]))[0] #area
        bi3 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[15]))[0] #bathrooms
        bi4 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[19]))[0] #bedrooms
        bi5 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[8]))[0] #use
        bi6 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_buildings&#39;).findAll(&#39;span&#39;)[6]))[0] #builduse

        #Find the assessment details (ad)
        ad0 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;LandDetails&#39;).findAll(&#39;td&#39;)[0]))[0] #fair market value
        ad1 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;LandDetails&#39;).findAll(&#39;td&#39;)[1]))[0] #land assessed value
        ad2 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;LandDetails&#39;).findAll(&#39;td&#39;)[2]))[0] #acres
        ad3 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[-3]))[0] #improvement
        ad4 = re.findall(r&#39;&amp;gt;(.*?)&amp;lt;&#39;, str(soup_ID.find(id = &#39;default_overview&#39;).findAll(&#39;span&#39;)[-1]))[0] #totalfmv
        
        #write sql code to insert
        sqltemp_ki = &amp;quot;&amp;quot;&amp;quot;INSERT INTO keyinfo(landuse,subdiv,saledate,saleprice,id)
             VALUES(%s,%s,%s,%s,%s);&amp;quot;&amp;quot;&amp;quot;
        
        sqltemp_bi = &amp;quot;&amp;quot;&amp;quot;INSERT INTO buildinfo(value,year,area,bathrooms,bedrooms,use,builduse,id)
             VALUES(%s,%s,%s,%s,%s,%s,%s,%s);&amp;quot;&amp;quot;&amp;quot;
        
        sqltemp_ad = &amp;quot;&amp;quot;&amp;quot;INSERT INTO assessment(fmv,lav,acres,improvement,totalfmv,id)
             VALUES(%s,%s,%s,%s,%s,%s);&amp;quot;&amp;quot;&amp;quot;
        
        #insert data in to sql
        callsql(sqltemp_ki, [ki0,ki1,ki2,ki3,i])
        callsql(sqltemp_bi, [bi0,bi1,bi2,bi3,bi4,bi5,bi6,i])
        callsql(sqltemp_ad, [ad0,ad1,ad2,ad3,ad4,i])
    
    driver.quit()  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4 id=&#34;step-3&#34;&gt;Step 3&lt;/h4&gt;
&lt;p&gt;Now we are ready to run these functions and start collecting some data to put in to our database for later.&lt;/p&gt;
&lt;p&gt;Before we start, we are going to define to sql queries to update the status in out &lt;code&gt;scrapeprog&lt;/code&gt; table, to let us know if we scraped the data successfully or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##Define the calls for if we sucessfully/un scrape data##

#Update the ID status to 1 = success
sqlsuccess = sql.SQL(&amp;quot;UPDATE scrapeprog SET status = 1 WHERE id = ({})&amp;quot;).format(sql.Literal(str(j[0])))

#Update the ID status to 0 = fail
sqlfail = sql.SQL(&amp;quot;UPDATE scrapeprog SET status = 0 WHERE id = ({})&amp;quot;).format(sql.Literal(str(j[0])))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we run the &lt;code&gt;getpropertydata()&lt;/code&gt; function in a while loop until there are no &amp;lsquo;pending&amp;rsquo; ID values in the &lt;code&gt;scrapeprog&lt;/code&gt; SQL table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##Run this in a while loop until it cant run no more!##
while findsql()[0][1] == 2: #i.e. while status is pending = 2
    j = findsql()[0] #where the 0th index is the id, and the 1st is the status
    try:
        getpropertydata(j[0]) 
        callsql(sqlsuccess)
    except:
        print(j[0]) #just a nice visual as we go
        callsql(sqlfail)  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id=&#34;future-uses&#34;&gt;Future Uses&lt;/h3&gt;
&lt;p&gt;In another blog post sometime, I am going to use the data I collected to run an isolation forest algorithm to detect outliers in the assessment value of each property, based on the features of each property. These outliers could be due to errors in data input (the county website has a disclaimer regarding this), human error in the assessment of a property, or particularly odd properties that have unknown features not captured in the property tax records.&lt;/p&gt;
&lt;p&gt;I also hope to find a few more uses for this information, likely involving predicting sales data for properties or determining what features of a property (i.e. bedrooms, bathrooms, acreage, etc) increase its value the most. These are just some rough ideas though.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3 id=&#34;reading-list&#34;&gt;Reading List&lt;/h3&gt;
&lt;h4 id=&#34;ethical-issues&#34;&gt;Ethical Issues&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ethics in Web Scraping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.apa.org/ethics/code/code-1992&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ethical Principles of Psychologists and Code of Conduct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://link.springer.com/article/10.1023/B:ETIN.0000047476.05912.3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ethical issues in web data mining&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;technical-reading&#34;&gt;Technical Reading&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.postgresqltutorial.com/postgresql-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PostgreSQL in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://realpython.com/beautiful-soup-web-scraper-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beautiful Soup: Build a Web Scraper With Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://levelup.gitconnected.com/web-scraping-with-selenium-in-python-8fde2f0fd559&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web Scraping with Selenium in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>HMDA Loan Application - An SQL exercise</title>
      <link>/post/blog1/</link>
      <pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/blog1/</guid>
      <description>


&lt;p&gt;Today we are going to use HMDA loan application data to look at the relationship between different demographic features of borrowers when applying for a loan, and how that impacts eventual loan acceptance or denial in 2008. We are also going to look at how these demographic factors interact with different reasons given for loan denial. Before we begin, this is just a quick example of using SQL databases in your analysis pipelines, so while we are looking at real data about real people, other researchers and scholars have done a lot more complex and interesting work on this topic and I encourage you to check them out (&lt;a href=&#34;#reading-list&#34;&gt;click here for reading list&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We are going ask two questions of this dataset today, and use SQL to query the dataset.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;div id=&#34;question-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Question 1&lt;/h3&gt;
&lt;p&gt;In Durham County, NC, does an applicant’s race affect loan denial rates, when no reason for denial is given?&lt;/p&gt;
&lt;div id=&#34;step-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1&lt;/h4&gt;
&lt;p&gt;First we need to connect our R notebook to the local SQL database. Thankfully, with the &lt;code&gt;RPostgres&lt;/code&gt; and &lt;code&gt;DBI&lt;/code&gt; packages, it is exceedingly simple to connect it to R in notebooks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#I always try to load all the packages I expect to use first and try to properly set up my R environment
library(DBI) #load the database interface package
library(dplyr) #load dplyr for later
library(ggplot2) # we will plot later

#all the below settings can be changed to work with cloud databases as well
con &amp;lt;- dbConnect(RPostgres::Postgres(), #you&amp;#39;ll need RPostgres for R to talk to PostgreSQL
                 dbname = &amp;#39;mortgages&amp;#39;, #this is database I created
                 host = &amp;#39;localhost&amp;#39;, #currently I have this saved locally
                 port = 5432, #localport
                 user = &amp;#39;postgres&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2&lt;/h4&gt;
&lt;p&gt;Now that we are connected to the database, we can send SQL queries in R either through SQL defined code chunks or sending commands in R code chunks using &lt;code&gt;tbl()&lt;/code&gt;. I’m going to opt for the former because I like to keep my languages seperate in my notebooks as much as possible.&lt;/p&gt;
&lt;p&gt;In this first query, we are going pull data for the primary applicant’s race, what action was taken on the loan application, and then compute the percent of applicantion outcomes by race, specifically only when there was no reason given for denial (if it was denied). I’m going to also involve some computations in my SQL query, so that it will calculate the average outcome (i.e. ACTIONTYPE) for an application by race (see the second line of code).&lt;/p&gt;
&lt;p&gt;As a quick technical note for the code, make sure the code chunk header looks like this: &lt;code&gt;{sql, connection=con, output.var=&amp;quot;df.q1&amp;quot;}&lt;/code&gt;. Here I have connected the code chunk to the database variable, and then I save the output of the SQL query to a variable I can plot later in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT RACE1, ACTIONTYPE, 
      COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(PARTITION BY RACE1) AS Percentage
FROM hmda
WHERE STATE=&amp;#39;37&amp;#39; AND COUNTY=&amp;#39;063&amp;#39; AND DENIAL1 = &amp;#39;&amp;#39;
GROUP BY ACTIONTYPE, RACE1;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 3&lt;/h4&gt;
&lt;p&gt;Now instead of just printing out a table, let’s plot the results for an easier interpretation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df.q1 %&amp;gt;% filter(race1 != 6, race1 != 7), #get rid of race when (1) not applicable and (2) when not provided by applicant
       aes(x = race1, y = percentage, 
           group = interaction(race1, factor(actiontype)), 
           fill = factor(actiontype))) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;quot;black&amp;quot;) + 
  ylab(&amp;quot;Percent of applications (%)&amp;quot;) + xlab(&amp;quot;Race&amp;quot;) + #set axis labels
  scale_fill_brewer(palette=&amp;quot;Dark2&amp;quot;, #use a default color scheme and set labels
                    name=&amp;quot;Action to Loan Application&amp;quot;, 
                    label = c(&amp;quot;Loan Originated&amp;quot;,
                              &amp;quot;Application approaced but not accepted&amp;quot;,
                              &amp;quot;Application denied by financial institution&amp;quot;,
                              &amp;quot;Application withdrawn by applicant&amp;quot;,
                              &amp;quot;File closed for incompleteness&amp;quot;,
                              &amp;quot;Loan purchased by institution&amp;quot;)) +
  scale_x_discrete(labels = c(&amp;quot;American Indian \nAlaska Native&amp;quot;,
                              &amp;quot;Asian&amp;quot;,
                              &amp;quot;Black \nAfrican American&amp;quot;,
                              &amp;quot;Native Hawaiian \n Pacific Islander&amp;quot;,
                              &amp;quot;White&amp;quot;)) +
  theme_bw() + #set of design elements that look nice
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) #angle the x-axis labels to fit&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/blog1_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each bar for race adds up to 100% because we want to know what happens to applications as a funciton of race, but we need to account for different base rates of applications. Therefore, looking at the outcome for applications as a percent value for each race lets us do between-race comparisons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-this-mean&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What does this mean?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Most people (I hope) won’t be surprised (but dissapointed) to see that the percent of White applicants with approved loan applications is higher than most other racial demographics. This is especially true for Black applicants and American Indian/Alaska Native applicants, but also for Native Hawaiian/Pacific Islander applicants.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here is where interpretting data is especially important. You should also notice that in purple we see that the percent of applicants who are denied, &lt;strong&gt;for no given reason&lt;/strong&gt;, is much larger for Black and American Indian/Alaska Native appicants.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This leads us to further questions we might want to start asking (and that some researchers have written on &lt;a href=&#34;#reading-list&#34;&gt;see&lt;/a&gt;); why are Black and American Indian/Alaska Native applicants being denied at higher rates than White applicants for no stated reasons?
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;question-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Question 2&lt;/h3&gt;
&lt;p&gt;In Durham County, NC, is the income of accepted loan applications different based on race?&lt;/p&gt;
&lt;div id=&#34;step-1-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1&lt;/h4&gt;
&lt;p&gt;Here, I will change my output variable to reflect I am asking a new question: &lt;code&gt;{sql, connection=con, output.var=&amp;quot;df.q2&amp;quot;}&lt;/code&gt;. Here we can ask SQL to turn the income variable to a numeric value we can average over using &lt;code&gt;CAST&lt;/code&gt;. This will throw an error if we don’t remove the NA values from INCOME in our join.&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT RACE1, AVG(CAST(INCOME AS NUMERIC))
FROM hmda
WHERE STATE=&amp;#39;37&amp;#39; AND COUNTY=&amp;#39;063&amp;#39; AND ACTIONTYPE = &amp;#39;1&amp;#39; AND INCOME &amp;lt;&amp;gt; &amp;#39;NA&amp;#39;
GROUP BY RACE1;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2&lt;/h4&gt;
&lt;p&gt;Now let’s plot the results of the SQL query. The code for the plot here is pretty similar to above, so I’ve hidden it this time.
&lt;img src=&#34;/post/blog1_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-this-mean-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What does this mean?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll notice that Black applicants had the lowest average income in Durham County, NC, among accepted loan applications. This is a stark difference in the graph, and there is a lot more at play here that contributes to this disparity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, though White applicants had the highest acceptance rate for loan applications, you’ll notice that they do not have the highest average income. In fact, the average income for the remaining racial groups is within 10k of each other.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-am-i-doing-this&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why am I doing this?&lt;/h3&gt;
&lt;p&gt;I’ve been properly learning SQL through Coursera course (&lt;a href=&#34;https://www.coursera.org/learn/analytics-mysql/&#34;&gt;here&lt;/a&gt;) this summer. A few years ago, I quickly loaded up some databases for a perception and decision making project I was doing simulations for, but in the hurry of trying to get out a finished product I didn’t have the time to properly learn the fundementals of the language. It turns out, it is quite useful (surprising no data scientist ever), especially when you start working with very large amounts of data. Obviously SQL databases are indispensible for many businesses in record keeping and other functions, but here I’m going to use them in a more exploratory research example when downloading data from Data.gov. There are a lot of rich, interesting datasets there, however, loading up a ~2.5Gb file in to R (or Python) seems excessive when I only need part of that dataset to answer the questions I’m interested in today.&lt;/p&gt;
&lt;p&gt;As a first pass – a little exercise outside of class – I logged on to Data.gov and just downloaded the first dataset I saw that seemed interesting; the 2008 Home Mortgage Disclosure Act (HMDA) Loan Application Register (LAR) Data (&lt;a href=&#34;https://catalog.data.gov/dataset/2008-home-mortgage-disclosure-act-hmda-loan-application-register-lar-data&#34;&gt;see&lt;/a&gt;). For this example, I just quickly converted this massive csv into a database using PostgreSQL (&lt;a href=&#34;https://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/&#34;&gt;see&lt;/a&gt;). Though today I only asked simple questions using this data, if I want, I can keep building this database as I find and get more data from different sources. This allows me to store my gathered data in a much more dynamic framework, allowing for the addition of more datasets, without having a bunch of messy and weirdly named .csv files sitting in a folder. Open and well-kept data is good data.
&lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-list&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reading List&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-11711-5_11&#34;&gt;A Loan at Last? Race and Racism in Mortgage Lending&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/10511482.2017.1341944?casa_token=b3GMq9t6wLsAAAAA%3AaLUQuyPbjXkI9KXZib0FAg91qA74Hw2YDADejjIrmm90o_XqGeYkAJG6wKM-L1F6g0SiD16me7MX&#34;&gt;Segregation and the Geography of Creditworthiness: Racial Inequality in a Recovered Mortgage Market&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.questia.com/library/journal/1G1-484628739/residential-mortgage-lending-from-2004-to-2015-evidence&#34;&gt;Residential Mortgage Lending from 2004 to 2015: Evidence from the Home Mortgage Disclosure Act Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/26590562?seq=1#metadata_info_tab_contents&#34;&gt;Big Data and Discrimination&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
