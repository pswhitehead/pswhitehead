[{"authors":["admin"],"categories":null,"content":"I specialize in using an interdisciplinary approach to deliver actionable insights into human behavior and decision making. As a PhD researcher, I primarily focused on the basic processes of human performance, attention, and memory.\nI am highly proficient in different methodologies for experimental design and collection (EEG, eye tracking, individual differences, fMRI, online behavioral assessment) and the creation of custom pipelines for the analysis and interpretation of data using various techniques (hierarchical modeling, multivariate decoding, signal processing, Bayesian analysis).\nMy work has been featured in top-tier academic journals such as the Journal of Cognitive Neuroscience and Cognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/peter-whitehead/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peter-whitehead/","section":"authors","summary":"I specialize in using an interdisciplinary approach to deliver actionable insights into human behavior and decision making. As a PhD researcher, I primarily focused on the basic processes of human performance, attention, and memory.","tags":null,"title":"Peter Whitehead","type":"authors"},{"authors":null,"categories":null,"content":" This week I decided to write a pretty simple webscraper to collect some property records from the Durham County website. I looked to see if they had an API, but I couldn’t find one, so this seemed like an opportunity to write a short blog about building a quick webscraper (reading for technical resources).\nThis script will build a database of property values and information that I can use for future analysis and projects (thoughts on future uses). Since I view this as ‘human data’, at least in some part, I’m only going to scrape non-identifiable data (articles on ethics in data mining); I won’t be collecting the physical address or owner’s name for each property. While I will go over the code here, you can download the python code from my repository here. \nMain Goal The main goal today when building this webscraper is to load and collect relevant property record data from each parcel in Durham county, from the county website, and then input that data in to a PostgreSQL database. Below I will walk through each section of the code to do this. \nStep 1 We need to import all the libraries we need for this project. I am using Selenium (link), as the website I’m scraping from needs to load up the webpage for us to scrape the information. I also chose to use psycopg2 (link) to interact with the PostgreSQL database I created for this project. If you need to install Selenium or psycopg2, it’s pretty easy to do via Anaconda (link).\n####Import the libraries we need#### import numpy as np from selenium import webdriver from bs4 import BeautifulSoup import time import re from selenium.webdriver.firefox.options import Options import psycopg2 from psycopg2 import sql \n Step 2 Once we have the libraries loaded up we are going to write the functions we need both to interact with the SQL database as well as scrape the data from the county website.\nIn the SQL database I created, I have a table – scrapeprog – that contains all the url IDs and a status variable for each, currently set to 2 which will represent a ‘pending’ status here. This first SQL call function is going to allow us to run the .py script without needing to give an input by querying the scrapeprog table in our SQL database to see which url ID we haven’t tried to scrape yet.\n##To query what we need to scrape yet## def findsql(): #Set up a connection to the PostgresSQL server we\u0026#39;ll write data to conn = psycopg2.connect(host=\u0026quot;localhost\u0026quot;,database=\u0026quot;durhamprop\u0026quot;, user=\u0026quot;postgres\u0026quot;, password=\u0026quot;postgres\u0026quot;) cur = conn.cursor() #Write an sql query query = sql.SQL(\u0026quot;SELECT * FROM {table} WHERE {key} = 2 LIMIT 1\u0026quot;).format( table=sql.Identifier(\u0026#39;scrapeprog\u0026#39;), key=sql.Identifier(\u0026#39;status\u0026#39;)) #execute query cur.execute(query) return cur.fetchall() The second SQL call function will allow us to insert and edit data in the SQL database.\n##To make sql calls to insert and change data## def callsql(command, data = None): #Set up a connection to the PostgresSQL server we\u0026#39;ll write data to conn = psycopg2.connect(host=\u0026quot;localhost\u0026quot;,database=\u0026quot;durhamprop\u0026quot;, user=\u0026quot;postgres\u0026quot;, password=\u0026quot;postgres\u0026quot;) cur = conn.cursor() #execute the query if data == None: cur.execute(command) else: cur.execute(command, data) conn.commit()  The third function I wrote allows us to load a webpage from the county’s property records site and scrape the data we want to put in the SQL database\n##To scrape the data from each property## def getpropertydata(i): #Setp up the website we\u0026#39;ll scrape data from url = [\u0026quot;https://property.spatialest.com/nc/durham/#/property/\u0026quot;+str(i)] #Set up the Selenium web driver and then call the URL options = Options() options.headless = True #I don\u0026#39;t want to open a physical window each tme driver = webdriver.Firefox(options=options) driver.get(url[0]) time.sleep(2) #Because we are loading the page, I wait a few seconds before the next command if driver.current_url == url[0]: #check the status isn\u0026#39;t 404 or redirected before we scrape the info #Parse the html soup_ID = BeautifulSoup(driver.page_source, \u0026#39;html.parser\u0026#39;) #Find the key info (ki) ki0 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[9]))[0] #landuse ki1 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[13]))[0] #subdiv ki2 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[21]))[0] #saledate ki3 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[23]))[0] #saleprice #Find the buildinfo (bi) bi0 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[-1]))[0] #value bi1 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[4]))[0] #year built bi2 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[13]))[0] #area bi3 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[15]))[0] #bathrooms bi4 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[19]))[0] #bedrooms bi5 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[8]))[0] #use bi6 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_buildings\u0026#39;).findAll(\u0026#39;span\u0026#39;)[6]))[0] #builduse #Find the assessment details (ad) ad0 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;LandDetails\u0026#39;).findAll(\u0026#39;td\u0026#39;)[0]))[0] #fair market value ad1 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;LandDetails\u0026#39;).findAll(\u0026#39;td\u0026#39;)[1]))[0] #land assessed value ad2 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;LandDetails\u0026#39;).findAll(\u0026#39;td\u0026#39;)[2]))[0] #acres ad3 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[-3]))[0] #improvement ad4 = re.findall(r\u0026#39;\u0026gt;(.*?)\u0026lt;\u0026#39;, str(soup_ID.find(id = \u0026#39;default_overview\u0026#39;).findAll(\u0026#39;span\u0026#39;)[-1]))[0] #totalfmv #write sql code to insert sqltemp_ki = \u0026quot;\u0026quot;\u0026quot;INSERT INTO keyinfo(landuse,subdiv,saledate,saleprice,id) VALUES(%s,%s,%s,%s,%s);\u0026quot;\u0026quot;\u0026quot; sqltemp_bi = \u0026quot;\u0026quot;\u0026quot;INSERT INTO buildinfo(value,year,area,bathrooms,bedrooms,use,builduse,id) VALUES(%s,%s,%s,%s,%s,%s,%s,%s);\u0026quot;\u0026quot;\u0026quot; sqltemp_ad = \u0026quot;\u0026quot;\u0026quot;INSERT INTO assessment(fmv,lav,acres,improvement,totalfmv,id) VALUES(%s,%s,%s,%s,%s,%s);\u0026quot;\u0026quot;\u0026quot; #insert data in to sql callsql(sqltemp_ki, [ki0,ki1,ki2,ki3,i]) callsql(sqltemp_bi, [bi0,bi1,bi2,bi3,bi4,bi5,bi6,i]) callsql(sqltemp_ad, [ad0,ad1,ad2,ad3,ad4,i]) driver.quit()  \n Step 3 Now we are ready to run these functions and start collecting some data to put in to our database for later.\nBefore we start, we are going to define to sql queries to update the status in out scrapeprog table, to let us know if we scraped the data successfully or not.\n##Define the calls for if we sucessfully/un scrape data## #Update the ID status to 1 = success sqlsuccess = sql.SQL(\u0026quot;UPDATE scrapeprog SET status = 1 WHERE id = ({})\u0026quot;) #Update the ID status to 0 = fail sqlfail = sql.SQL(\u0026quot;UPDATE scrapeprog SET status = 0 WHERE id = ({})\u0026quot;) Now we run the getpropertydata() function in a while loop until there are no ‘pending’ ID values in the scrapeprog SQL table.\n##Run this in a while loop until it cant run no more!## while findsql()[0][1] == 2: #i.e. while status is pending = 2 j = findsql()[0] #where the 0th index is the id, and the 1st is the status try: getpropertydata(j[0]) callsql(sqlsuccess.format(sql.Literal(str(j[0])))) except: print(j[0]) #just a nice visual as we go callsql(sqlfail.format(sql.Literal(str(j[0]))))  \n  Future Uses In another blog post sometime, I am going to use the data I collected to run an isolation forest algorithm to detect outlier properties in Durham. These outliers could be due to errors in data input (the county website has a disclaimer regarding this), human error in the assessment of this property, or particularly odd features of a parcel. More on this later.\nI also hope to find a few more uses for this information, maybe involving predicting sales data for properties or determining what features of a property (i.e. bedrooms, bathrooms, acreage, etc) increase its value the most. These are just some rough ideas though. \n Reading List Ethical Issues  Ethics in Web Scraping Ethical Principles of Psychologists and Code of Conduct Ethical issues in web data mining   Technical Reading  PostgreSQL in Python Beautiful Soup: Build a Web Scraper With Python Web Scraping with Selenium in Python A beginner’s guide to web scraping with Python    ","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"7c5a4d4c5cc8616d5fec51caf8f8ce6a","permalink":"/post/blog2/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/post/blog2/","section":"post","summary":"This week I decided to write a pretty simple webscraper to collect some property records from the Durham County website. I looked to see if they had an API, but I couldn’t find one, so this seemed like an opportunity to write a short blog about building a quick webscraper (reading for technical resources).","tags":["SQL","python","PostgreSQL","webscraping","housing","multi-part blog"],"title":"Scraping Durham Property Records: Part I","type":"post"},{"authors":null,"categories":null,"content":" Today we are going to use HMDA loan application data to look at the relationship between different demographic features of borrowers when applying for a loan, and how that impacts eventual loan acceptance or denial in 2008. We are also going to look at how these demographic factors interact with different reasons given for loan denial. Before we begin, this is just a quick example of using SQL databases in your analysis pipelines, so while we are looking at real data about real people, other researchers and scholars have done a lot more complex and interesting work on this topic and I encourage you to check them out (click here for reading list).\nWe are going ask two questions of this dataset today, and use SQL to query the dataset. \nQuestion 1 In Durham County, NC, does an applicant’s race affect loan denial rates, when no reason for denial is given?\nStep 1 First we need to connect our R notebook to the local SQL database. Thankfully, with the RPostgres and DBI packages, it is exceedingly simple to connect it to R in notebooks.\n#I always try to load all the packages I expect to use first and try to properly set up my R environment library(DBI) #load the database interface package library(dplyr) #load dplyr for later library(ggplot2) # we will plot later #all the below settings can be changed to work with cloud databases as well con \u0026lt;- dbConnect(RPostgres::Postgres(), #you\u0026#39;ll need RPostgres for R to talk to PostgreSQL dbname = \u0026#39;mortgages\u0026#39;, #this is database I created host = \u0026#39;localhost\u0026#39;, #currently I have this saved locally port = 5432, #localport user = \u0026#39;postgres\u0026#39;)  Step 2 Now that we are connected to the database, we can send SQL queries in R either through SQL defined code chunks or sending commands in R code chunks using tbl(). I’m going to opt for the former because I like to keep my languages seperate in my notebooks as much as possible.\nIn this first query, we are going pull data for the primary applicant’s race, what action was taken on the loan application, and then compute the percent of applicantion outcomes by race, specifically only when there was no reason given for denial (if it was denied). I’m going to also involve some computations in my SQL query, so that it will calculate the average outcome (i.e. ACTIONTYPE) for an application by race (see the second line of code).\nAs a quick technical note for the code, make sure the code chunk header looks like this: {sql, connection=con, output.var=\u0026quot;df.q1\u0026quot;}. Here I have connected the code chunk to the database variable, and then I save the output of the SQL query to a variable I can plot later in ggplot2.\nSELECT RACE1, ACTIONTYPE, COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(PARTITION BY RACE1) AS Percentage FROM hmda WHERE STATE=\u0026#39;37\u0026#39; AND COUNTY=\u0026#39;063\u0026#39; AND DENIAL1 = \u0026#39;\u0026#39; GROUP BY ACTIONTYPE, RACE1;  Step 3 Now instead of just printing out a table, let’s plot the results for an easier interpretation.\nggplot(df.q1 %\u0026gt;% filter(race1 != 6, race1 != 7), #get rid of race when (1) not applicable and (2) when not provided by applicant aes(x = race1, y = percentage, group = interaction(race1, factor(actiontype)), fill = factor(actiontype))) + geom_bar(stat = \u0026#39;identity\u0026#39;, color = \u0026quot;black\u0026quot;) + ylab(\u0026quot;Percent of applications (%)\u0026quot;) + xlab(\u0026quot;Race\u0026quot;) + #set axis labels scale_fill_brewer(palette=\u0026quot;Dark2\u0026quot;, #use a default color scheme and set labels name=\u0026quot;Action to Loan Application\u0026quot;, label = c(\u0026quot;Loan Originated\u0026quot;, \u0026quot;Application approaced but not accepted\u0026quot;, \u0026quot;Application denied by financial institution\u0026quot;, \u0026quot;Application withdrawn by applicant\u0026quot;, \u0026quot;File closed for incompleteness\u0026quot;, \u0026quot;Loan purchased by institution\u0026quot;)) + scale_x_discrete(labels = c(\u0026quot;American Indian \\nAlaska Native\u0026quot;, \u0026quot;Asian\u0026quot;, \u0026quot;Black \\nAfrican American\u0026quot;, \u0026quot;Native Hawaiian \\n Pacific Islander\u0026quot;, \u0026quot;White\u0026quot;)) + theme_bw() + #set of design elements that look nice theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) #angle the x-axis labels to fit Each bar for race adds up to 100% because we want to know what happens to applications as a funciton of race, but we need to account for different base rates of applications. Therefore, looking at the outcome for applications as a percent value for each race lets us do between-race comparisons.\n What does this mean?  Most people (I hope) won’t be surprised (but dissapointed) to see that the percent of White applicants with approved loan applications is higher than most other racial demographics. This is especially true for Black applicants and American Indian/Alaska Native applicants, but also for Native Hawaiian/Pacific Islander applicants.\n Here is where interpretting data is especially important. You should also notice that in purple we see that the percent of applicants who are denied, for no given reason, is much larger for Black and American Indian/Alaska Native appicants.\n This leads us to further questions we might want to start asking (and that some researchers have written on see); why are Black and American Indian/Alaska Native applicants being denied at higher rates than White applicants for no stated reasons? \n    Question 2 In Durham County, NC, is the income of accepted loan applications different based on race?\nStep 1 Here, I will change my output variable to reflect I am asking a new question: {sql, connection=con, output.var=\u0026quot;df.q2\u0026quot;}. Here we can ask SQL to turn the income variable to a numeric value we can average over using CAST. This will throw an error if we don’t remove the NA values from INCOME in our join.\nSELECT RACE1, AVG(CAST(INCOME AS NUMERIC)) FROM hmda WHERE STATE=\u0026#39;37\u0026#39; AND COUNTY=\u0026#39;063\u0026#39; AND ACTIONTYPE = \u0026#39;1\u0026#39; AND INCOME \u0026lt;\u0026gt; \u0026#39;NA\u0026#39; GROUP BY RACE1;  Step 2 Now let’s plot the results of the SQL query. The code for the plot here is pretty similar to above, so I’ve hidden it this time.  What does this mean?  You’ll notice that Black applicants had the lowest average income in Durham County, NC, among accepted loan applications. This is a stark difference in the graph, and there is a lot more at play here that contributes to this disparity.\n However, though White applicants had the highest acceptance rate for loan applications, you’ll notice that they do not have the highest average income. In fact, the average income for the remaining racial groups is within 10k of each other. \n    Why am I doing this? I’ve been properly learning SQL through Coursera course (here) this summer. A few years ago, I quickly loaded up some databases for a perception and decision making project I was doing simulations for, but in the hurry of trying to get out a finished product I didn’t have the time to properly learn the fundementals of the language. It turns out, it is quite useful (surprising no data scientist ever), especially when you start working with very large amounts of data. Obviously SQL databases are indispensible for many businesses in record keeping and other functions, but here I’m going to use them in a more exploratory research example when downloading data from Data.gov. There are a lot of rich, interesting datasets there, however, loading up a ~2.5Gb file in to R (or Python) seems excessive when I only need part of that dataset to answer the questions I’m interested in today.\nAs a first pass – a little exercise outside of class – I logged on to Data.gov and just downloaded the first dataset I saw that seemed interesting; the 2008 Home Mortgage Disclosure Act (HMDA) Loan Application Register (LAR) Data (see). For this example, I just quickly converted this massive csv into a database using PostgreSQL (see). Though today I only asked simple questions using this data, if I want, I can keep building this database as I find and get more data from different sources. This allows me to store my gathered data in a much more dynamic framework, allowing for the addition of more datasets, without having a bunch of messy and weirdly named .csv files sitting in a folder. Open and well-kept data is good data. \n Reading List  A Loan at Last? Race and Racism in Mortgage Lending Segregation and the Geography of Creditworthiness: Racial Inequality in a Recovered Mortgage Market Residential Mortgage Lending from 2004 to 2015: Evidence from the Home Mortgage Disclosure Act Data Big Data and Discrimination   ","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"5ed80b93327491a805bf858b4e6f37bc","permalink":"/post/blog1/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/post/blog1/","section":"post","summary":"Today we are going to use HMDA loan application data to look at the relationship between different demographic features of borrowers when applying for a loan, and how that impacts eventual loan acceptance or denial in 2008.","tags":["SQL","R","Postgres","HMDA","linear model"],"title":"HMDA Loan Application - An SQL exercise","type":"post"}]